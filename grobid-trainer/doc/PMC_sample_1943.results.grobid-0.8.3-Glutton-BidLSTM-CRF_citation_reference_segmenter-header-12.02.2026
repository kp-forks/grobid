======= Header metadata =======Evaluation on 1943 random PDF files out of 1941 PDF (ratio 1.0).======= Strict Matching ======= (exact matches)===== Field-level results =====label                accuracy     precision    recall       f1           supportabstract             82.49        16.89        16.54        16.71        1911   authors              98.45        92.83        92.68        92.76        1941   first_author         99.29        96.8         96.65        96.73        1941   keywords             94.27        65.58        63.91        64.73        1380   title                96.64        84.46        84.2         84.33        1943   all (micro avg.)     94.23        72.09        71.4         71.74        9116   all (macro avg.)     94.23        71.31        70.8         71.05        9116   ======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)===== Field-level results =====label                accuracy     precision    recall       f1           supportabstract             92.12        63.98        62.64        63.3         1911   authors              98.86        94.79        94.64        94.72        1941   first_author         99.38        97.21        97.06        97.14        1941   keywords             95.54        74.2         72.32        73.25        1380   title                98.24        92           91.71        91.86        1943   all (micro avg.)     96.83        85.26        84.44        84.85        9116   all (macro avg.)     96.83        84.44        83.68        84.05        9116   ==== Levenshtein Matching ===== (Minimum Levenshtein distance at 0.8)===== Field-level results =====label                accuracy     precision    recall       f1           supportabstract             97.62        90.86        88.96        89.9         1911   authors              99.26        96.65        96.5         96.57        1941   first_author         99.43        97.47        97.32        97.4         1941   keywords             97.07        84.61        82.46        83.52        1380   title                99.56        98.24        97.94        98.09        1943   all (micro avg.)     98.59        94.17        93.28        93.72        9116   all (macro avg.)     98.59        93.57        92.64        93.1         9116   = Ratcliff/Obershelp Matching = (Minimum Ratcliff/Obershelp similarity at 0.95)===== Field-level results =====label                accuracy     precision    recall       f1           supportabstract             96.84        87.07        85.24        86.14        1911   authors              99.06        95.72        95.57        95.64        1941   first_author         99.29        96.8         96.65        96.73        1941   keywords             96.38        79.93        77.9         78.9         1380   title                99.13        96.18        95.88        96.03        1943   all (micro avg.)     98.14        91.9         91.03        91.46        9116   all (macro avg.)     98.14        91.14        90.25        90.69        9116   ===== Instance-level results =====Total expected instances: 	1943Total correct instances: 	216 (strict) Total correct instances: 	907 (soft) Total correct instances: 	1445 (Levenshtein) Total correct instances: 	1298 (ObservedRatcliffObershelp) Instance-level recall:	11.12	(strict) Instance-level recall:	46.68	(soft) Instance-level recall:	74.37	(Levenshtein) Instance-level recall:	66.8	(RatcliffObershelp) ======= Citation metadata ======= Evaluation on 1943 random PDF files out of 1941 PDF (ratio 1.0).======= Strict Matching ======= (exact matches)===== Field-level results =====label                accuracy     precision    recall       f1           supportauthors              97.59        83.14        75.97        79.39        85778  date                 99.25        94.71        83.84        88.94        87067  first_author         98.54        89.87        82.1         85.81        85778  inTitle              96.2         73.29        71.47        72.37        81007  issue                99.69        91.45        87.44        89.4         16635  page                 98.63        94.69        83.32        88.64        80501  title                97.23        79.84        75.01        77.35        80736  volume               99.46        96.18        89.38        92.65        80067  all (micro avg.)     98.32        87.34        80.36        83.71        597569 all (macro avg.)     98.32        87.89        81.07        84.32        597569 ======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)===== Field-level results =====label                accuracy     precision    recall       f1           supportauthors              97.66        83.61        76.4         79.84        85778  date                 99.25        94.71        83.84        88.94        87067  first_author         98.57        90.04        82.26        85.97        85778  inTitle              97.87        85.05        82.95        83.99        81007  issue                99.69        91.45        87.44        89.4         16635  page                 98.63        94.69        83.32        88.64        80501  title                98.84        91.62        86.08        88.76        80736  volume               99.46        96.18        89.38        92.65        80067  all (micro avg.)     98.74        90.75        83.5         86.97        597569 all (macro avg.)     98.74        90.92        83.96        87.27        597569 ==== Levenshtein Matching ===== (Minimum Levenshtein distance at 0.8)===== Field-level results =====label                accuracy     precision    recall       f1           supportauthors              98.47        89.31        81.62        85.29        85778  date                 99.25        94.71        83.84        88.94        87067  first_author         98.6         90.25        82.46        86.18        85778  inTitle              98.05        86.3         84.16        85.21        81007  issue                99.69        91.45        87.44        89.4         16635  page                 98.63        94.69        83.32        88.64        80501  title                99.15        93.92        88.23        90.99        80736  volume               99.46        96.18        89.38        92.65        80067  all (micro avg.)     98.91        92.09        84.73        88.26        597569 all (macro avg.)     98.91        92.1         85.06        88.41        597569 = Ratcliff/Obershelp Matching = (Minimum Ratcliff/Obershelp similarity at 0.95)===== Field-level results =====label                accuracy     precision    recall       f1           supportauthors              98.01        86.08        78.66        82.2         85778  date                 99.25        94.71        83.84        88.94        87067  first_author         98.54        89.88        82.12        85.83        85778  inTitle              97.67        83.61        81.54        82.57        81007  issue                99.69        91.45        87.44        89.4         16635  page                 98.63        94.69        83.32        88.64        80501  title                99.1         93.53        87.87        90.61        80736  volume               99.46        96.18        89.38        92.65        80067  all (micro avg.)     98.79        91.14        83.85        87.34        597569 all (macro avg.)     98.79        91.27        84.27        87.61        597569 ===== Instance-level results =====Total expected instances: 		90125Total extracted instances: 		85142Total correct instances: 		38579 (strict) Total correct instances: 		50695 (soft) Total correct instances: 		55504 (Levenshtein) Total correct instances: 		52077 (RatcliffObershelp) Instance-level precision:	45.31 (strict) Instance-level precision:	59.54 (soft) Instance-level precision:	65.19 (Levenshtein) Instance-level precision:	61.16 (RatcliffObershelp) Instance-level recall:	42.81	(strict) Instance-level recall:	56.25	(soft) Instance-level recall:	61.59	(Levenshtein) Instance-level recall:	57.78	(RatcliffObershelp) Instance-level f-score:	44.02 (strict) Instance-level f-score:	57.85 (soft) Instance-level f-score:	63.34 (Levenshtein) Instance-level f-score:	59.43 (RatcliffObershelp) Matching 1 :	68048Matching 2 :	4086Matching 3 :	1859Matching 4 :	659Total matches :	74652======= Citation context resolution ======= Total expected references: 	 90125 - 46.38 references per articleTotal predicted references: 	 85142 - 43.82 references per articleTotal expected citation contexts: 	 139835 - 71.97 citation contexts per articleTotal predicted citation contexts: 	 114545 - 58.95 citation contexts per articleTotal correct predicted citation contexts: 	 97018 - 49.93 citation contexts per articleTotal wrong predicted citation contexts: 	 17527 (wrong callout matching, callout missing in NLM, or matching with a bib. ref. not aligned with a bib.ref. in NLM)Precision citation contexts: 	 84.7Recall citation contexts: 	 69.38fscore citation contexts: 	 76.28======= Fulltext structures ======= Evaluation on 1943 random PDF files out of 1941 PDF (ratio 1.0).======= Strict Matching ======= (exact matches)===== Field-level results =====label                accuracy     precision    recall       f1           supportfigure_title         96.57        31.57        26.62        28.88        7281   reference_citation   59.71        58.13        58.76        58.45        134196 reference_figure     94.71        60.6         68.28        64.21        19330  reference_table      99.24        82.87        89.55        86.08        7327   section_title        94.39        73.61        67.77        70.57        27619  table_title          98.94        67.77        49.61        57.28        3971   all (micro avg.)     90.59        60.67        60.71        60.69        199724 all (macro avg.)     90.59        62.43        60.1         60.91        199724 ======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)===== Field-level results =====label                accuracy     precision    recall       f1           supportfigure_title         98.65        79.57        67.08        72.79        7281   reference_citation   62.37        62.41        63.09        62.75        134196 reference_figure     94.57        61.1         68.85        64.74        19330  reference_table      99.22        83.05        89.74        86.26        7327   section_title        95.23        79.12        72.84        75.85        27619  table_title          99.48        94.22        68.98        79.65        3971   all (micro avg.)     91.59        66.2         66.24        66.22        199724 all (macro avg.)     91.59        76.58        71.76        73.67        199724 ===== Document-level ratio results =====label                accuracy     precision    recall       f1           supportall (micro avg.)     0            0            0            0            0      all (macro avg.)     0            0            0            0            0      ====================================================================================Evaluation report in markdown format saved under /data/workspace/services/grobid/grobid-trainer/../grobid-home/tmp/report.mdBUILD SUCCESSFUL in 20m 36s7 actionable tasks: 1 executed, 6 up-to-date