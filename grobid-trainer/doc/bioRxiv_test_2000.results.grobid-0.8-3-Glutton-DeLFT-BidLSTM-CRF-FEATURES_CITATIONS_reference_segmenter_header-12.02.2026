
> Configure project :
Checking git under /data/workspace/services/grobid
Current Git revision: 0.8.2-43-g96741f13e

> Task :grobid-core:checkKotlinGradlePluginConfigurationErrors SKIPPED
> Task :grobid-core:compileKotlin

> Task :grobid-core:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.

> Task :grobid-core:processResources UP-TO-DATE
> Task :grobid-core:classes
> Task :grobid-core:jar
> Task :grobid-trainer:checkKotlinGradlePluginConfigurationErrors SKIPPED
> Task :grobid-trainer:compileKotlin NO-SOURCE
> Task :grobid-trainer:compileJava UP-TO-DATE
> Task :grobid-trainer:processResources UP-TO-DATE
> Task :grobid-trainer:classes UP-TO-DATE

> Task :grobid-trainer:jatsEval
Flavor was not specified, or was empty. Using default Grobid process. 
>>>>>>>> GROBID_HOME=/data/workspace/services/grobid/grobid-trainer/../grobid-home

Evaluation metrics produced in 1598.654 seconds

======= Header metadata ======= 

Evaluation on 2000 random PDF files out of 1998 PDF (ratio 1.0).

======= Strict Matching ======= (exact matches)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

abstract             78.02        2.31         2.26         2.29         1989   
authors              96.5         85.17        84.48        84.82        1998   
first_author         99.14        96.92        96.24        96.58        1996   
keywords             95.74        57.56        58.59        58.07        838    
title                94.72        77.33        76.64        76.98        1999   

all (micro avg.)     92.83        64.94        64.37        64.65        8820   
all (macro avg.)     92.83        63.86        63.64        63.75        8820   


======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

abstract             90.67        59.93        58.57        59.24        1989   
authors              96.59        85.57        84.88        85.23        1998   
first_author         99.19        97.12        96.44        96.78        1996   
keywords             96.24        62.72        63.84        63.28        838    
title                95.21        79.51        78.79        79.15        1999   

all (micro avg.)     95.58        78.88        78.19        78.53        8820   
all (macro avg.)     95.58        76.97        76.51        76.73        8820   


==== Levenshtein Matching ===== (Minimum Levenshtein distance at 0.8)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

abstract             95.17        80.4         78.58        79.48        1989   
authors              98.18        92.68        91.94        92.31        1998   
first_author         99.24        97.38        96.69        97.03        1996   
keywords             97.83        79.25        80.67        79.95        838    
title                98.01        92.02        91.2         91.61        1999   

all (micro avg.)     97.69        89.56        88.76        89.16        8820   
all (macro avg.)     97.69        88.35        87.82        88.08        8820   


= Ratcliff/Obershelp Matching = (Minimum Ratcliff/Obershelp similarity at 0.95)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

abstract             94.36        76.75        75.01        75.87        1989   
authors              97.3         88.75        88.04        88.39        1998   
first_author         99.14        96.92        96.24        96.58        1996   
keywords             97.02        70.81        72.08        71.44        838    
title                97.04        87.68        86.89        87.29        1999   

all (micro avg.)     96.97        85.94        85.18        85.56        8820   
all (macro avg.)     96.97        84.18        83.65        83.91        8820   

===== Instance-level results =====

Total expected instances: 	1999
Total correct instances: 	38 (strict) 
Total correct instances: 	716 (soft) 
Total correct instances: 	1231 (Levenshtein) 
Total correct instances: 	1054 (ObservedRatcliffObershelp) 

Instance-level recall:	1.9	(strict) 
Instance-level recall:	35.82	(soft) 
Instance-level recall:	61.58	(Levenshtein) 
Instance-level recall:	52.73	(RatcliffObershelp) 

======= Citation metadata ======= 

Evaluation on 2000 random PDF files out of 1998 PDF (ratio 1.0).

======= Strict Matching ======= (exact matches)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

authors              98.41        88.22        83.08        85.57        97132  
date                 98.86        91.69        86.08        88.8         97579  
doi                  99.12        70.8         83.66        76.69        16894  
first_author         99.31        95.09        89.48        92.2         97132  
inTitle              97.69        82.84        79.2         80.98        96379  
issue                99.61        94.29        91.72        92.99        30312  
page                 97.51        94.99        78.06        85.7         88551  
pmcid                99.95        66.44        86.12        75.01        807    
pmid                 99.87        69.73        84.52        76.41        2093   
title                97.97        84.79        83.26        84.02        92415  
volume               99.46        96.21        94.94        95.57        87661  

all (micro avg.)     98.89        89.84        85.1         87.41        706955 
all (macro avg.)     98.89        85.01        85.46        84.9         706955 


======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

authors              98.56        89.38        84.17        86.69        97132  
date                 98.86        91.69        86.08        88.8         97579  
doi                  99.25        75.26        88.94        81.53        16894  
first_author         99.37        95.52        89.88        92.61        97132  
inTitle              98.97        92.32        88.27        90.25        96379  
issue                99.61        94.29        91.72        92.99        30312  
page                 97.51        94.99        78.06        85.7         88551  
pmcid                99.96        75.72        98.14        85.48        807    
pmid                 99.88        74.14        89.87        81.25        2093   
title                99.07        93.12        91.43        92.27        92415  
volume               99.46        96.21        94.94        95.57        87661  

all (micro avg.)     99.14        92.65        87.77        90.14        706955 
all (macro avg.)     99.14        88.42        89.23        88.47        706955 


==== Levenshtein Matching ===== (Minimum Levenshtein distance at 0.8)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

authors              99.26        94.63        89.12        91.79        97132  
date                 98.86        91.69        86.08        88.8         97579  
doi                  99.32        77.54        91.62        83.99        16894  
first_author         99.39        95.67        90.02        92.76        97132  
inTitle              99.1         93.33        89.23        91.23        96379  
issue                99.61        94.29        91.72        92.99        30312  
page                 97.51        94.99        78.06        85.7         88551  
pmcid                99.96        75.72        98.14        85.48        807    
pmid                 99.88        74.14        89.87        81.25        2093   
title                99.46        96.07        94.33        95.19        92415  
volume               99.46        96.21        94.94        95.57        87661  

all (micro avg.)     99.26        94           89.04        91.45        706955 
all (macro avg.)     99.26        89.48        90.28        89.52        706955 


= Ratcliff/Obershelp Matching = (Minimum Ratcliff/Obershelp similarity at 0.95)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

authors              98.85        91.59        86.25        88.84        97132  
date                 98.86        91.69        86.08        88.8         97579  
doi                  99.27        75.95        89.75        82.27        16894  
first_author         99.32        95.14        89.52        92.24        97132  
inTitle              98.8         91.08        87.08        89.04        96379  
issue                99.61        94.29        91.72        92.99        30312  
page                 97.51        94.99        78.06        85.7         88551  
pmcid                99.95        66.44        86.12        75.01        807    
pmid                 99.87        69.73        84.52        76.41        2093   
title                99.37        95.41        93.68        94.54        92415  
volume               99.46        96.21        94.94        95.57        87661  

all (micro avg.)     99.17        93.03        88.12        90.51        706955 
all (macro avg.)     99.17        87.5         87.97        87.4         706955 

===== Instance-level results =====

Total expected instances: 		98748
Total extracted instances: 		97659
Total correct instances: 		43524 (strict) 
Total correct instances: 		54446 (soft) 
Total correct instances: 		58679 (Levenshtein) 
Total correct instances: 		55424 (RatcliffObershelp) 

Instance-level precision:	44.57 (strict) 
Instance-level precision:	55.75 (soft) 
Instance-level precision:	60.09 (Levenshtein) 
Instance-level precision:	56.75 (RatcliffObershelp) 

Instance-level recall:	44.08	(strict) 
Instance-level recall:	55.14	(soft) 
Instance-level recall:	59.42	(Levenshtein) 
Instance-level recall:	56.13	(RatcliffObershelp) 

Instance-level f-score:	44.32 (strict) 
Instance-level f-score:	55.44 (soft) 
Instance-level f-score:	59.75 (Levenshtein) 
Instance-level f-score:	56.44 (RatcliffObershelp) 

Matching 1 :	78921

Matching 2 :	4556

Matching 3 :	4369

Matching 4 :	2097

Total matches :	89943

======= Citation context resolution ======= 

Total expected references: 	 98746 - 49.37 references per article
Total predicted references: 	 97659 - 48.83 references per article

Total expected citation contexts: 	 142776 - 71.39 citation contexts per article
Total predicted citation contexts: 	 134504 - 67.25 citation contexts per article

Total correct predicted citation contexts: 	 115917 - 57.96 citation contexts per article
Total wrong predicted citation contexts: 	 18587 (wrong callout matching, callout missing in NLM, or matching with a bib. ref. not aligned with a bib.ref. in NLM)

Precision citation contexts: 	 86.18
Recall citation contexts: 	 81.19
fscore citation contexts: 	 83.61

======= Fulltext structures ======= 

Evaluation on 2000 random PDF files out of 1998 PDF (ratio 1.0).

======= Strict Matching ======= (exact matches)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

availability_stmt    99.83        29.41        25.78        27.48        446    
figure_title         90.17        4.21         2.34         3.01         22967  
funding_stmt         98.54        3.63         23.16        6.28         747    
reference_citation   76.31        71.99        70.93        71.45        147384 
reference_figure     92.44        70.3         76.85        73.43        47896  
reference_table      98           45.19        84.98        59           5957   
section_title        94.26        68.82        68.57        68.69        32368  
table_title          98.53        6.94         2.55         3.73         3925   

all (micro avg.)     93.51        65.11        64.78        64.94        261690 
all (macro avg.)     93.51        37.56        44.39        39.13        261690 


======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)

===== Field-level results =====

label                accuracy     precision    recall       f1           support

availability_stmt    99.87        51.92        45.52        48.51        446    
figure_title         94.3         66.52        36.97        47.52        22967  
funding_stmt         98.41        3.84         24.5         6.65         747    
reference_citation   85.19        84.27        83.03        83.64        147384 
reference_figure     91.96        70.93        77.54        74.09        47896  
reference_table      97.85        45.59        85.71        59.52        5957   
section_title        94.83        74.24        73.98        74.11        32368  
table_title          99.07        81.53        29.91        43.77        3925   

all (micro avg.)     95.18        76.28        75.89        76.08        261690 
all (macro avg.)     95.18        59.85        57.14        54.72        261690 

===== Document-level ratio results =====

label                accuracy     precision    recall       f1           support

availability_stmt    66.72        84.82        87.67        86.22        446    

all (micro avg.)     66.72        84.82        87.67        86.22        446    
all (macro avg.)     66.72        84.82        87.67        86.22        446    

====================================================================================


Evaluation report in markdown format saved under /data/workspace/services/grobid/grobid-trainer/../grobid-home/tmp/report.md

[Incubating] Problems report is available at: file:///data/workspace/services/grobid/build/reports/problems/problems-report.html

Deprecated Gradle features were used in this build, making it incompatible with Gradle 10.

You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.

For more on this, please refer to https://docs.gradle.org/9.0.0/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.

BUILD SUCCESSFUL in 26m 55s
7 actionable tasks: 4 executed, 3 up-to-date
